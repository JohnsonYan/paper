%# -*- coding: utf-8-unix -*-

\chapter{深度学习和网络流量异常检测的基础理论}\label{chap:example}

\section{深度学习}

深度学习技术是机器学习领域研究的最新最火热的成果。自20世纪80年代开始，机器学习技术层出不穷，基于统计模型的感知器、人工神经网络等浅层神经网络，到后来的SVM、最大熵方法等在预测与分类上虽然取得了很好的应用效果，但是，随着互联网的快速发展，数据的数量和维度呈现爆炸式的增长，传统的浅层神经网络面对高维度、复杂数据时表现得力不从心。2006年，多伦多大学的Geoffrey Hinton教授联Ruslan Salakutdinov教授发表了一片开启了深度学习热潮的文章，使得机器学习技术进入了深度学习时代。这篇文章中有两个主要观点：1.多层人工神经网络对于特征学习问题有着非常良好的表现，通过深度学习技术学习到的特征可以对原始数据进行更加本质的刻画；2.深度学习可以克服传统的深层次神经网络在训练时面临的深层误差反馈的消减问题，深度学习可以通过逐层初始化的方法予以克服这一问题，同时，在逐层初始化的过程中，使用的是无监督学习。这使得在大数据时代，数据巨大而标签稀少的情况下，无监督学习的初始化方法有着很好的适用性。深度学习架构（例如DNN、DBN、RNN等）已经被应用至诸多领域，包括语音识别、计算机视觉、自然语言处理、社交网络过滤、音频识别、机器翻译、药物设计和生物信息学等诸多领域。与人类专家相比，在许多情况下它们已经达到了优于人类专家的水平。

\subsection{深度学习的基本思想}

深度学习虽然是计算机领域的技术，但是深度学习与20世纪90年代的由认知神经科学研究者提出的大脑发育理论(尤其是皮层发育理论)是密切相关的。经研究发现，我们人类的大脑的神经元组成了一种较为深层的网络结构，不同层的神经元肩负着不容的任务需要处理。例如，从视觉系统传输过来的信息需要经过多个层次的处理，我们的大脑才能得到正确结果。可以这样想象，在每一层中，我们的大脑的不同层次的神经元可以处理复杂程度不同的信息，浅层的神经元可以处理图像中的局部信息，深层的神经元可以处理图像中更大片区域的信息，信息通过神经元的逐层处理而不断地聚集，同时变得更易于我们大脑分析与处理。深度学习也是如此，如果想要让机器理解更加复杂的问题，模型也需要经过多个层次深度处理才可以。

深度学习是一种机器学习算法：
\begin{itemize}
    \item 适用多层非线性处理单元的级联进行特征提取和转换，每个连续的层使用前一层的输出作为当前层的输入。
    \item 在监督学习(例如：分类)和无监督学习(例如：模式分析)方式下训练。
    \item 学习对应于不同抽象级别的多级表示，这些多级表示构成了深度学习概念的层次结构。
\end{itemize}

深度学习使用了分层的思想，它逐层提取复杂的信号，假设一个具有$n$层的网络结构$A(A_1,A_2,\cdots,A_n)$，它的输入和输出分别是$I$和$O$，表示为：
\begin{equation}
    I \Rightarrow A_1 \Rightarrow A_2 \Rightarrow \cdots \Rightarrow A_n \Rightarrow O    
\end{equation}

如果输出$O$等于输入$I$，那么就可以认为这个模型在经过$n$层变化后是没有损失任何信息的。也就是说，其中的每一层都是原始信息的另一种抽象表示。我们的目的也就是使输入输出尽可能的相似，即输出与输出之间的误差最小。可以发现，这种模式下，深度学习和人的大脑的抽象过程是十分接近的，通过一层一层的抽象，将原始信息组合为机器所能够理解并处理的表示。深度学习的这种抽象是逐层训练的，避免了随着层数增加导致的计算复杂问题。同时，深度学习基于无监督学习的方法可以获取到数据的隐含特征，也并不需要巨量的标记好的数据支持。

深度学习的基本结构如图所示：
\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{structure.pdf}
    \caption{深度学习的基本结构}
    \label{fig:DLstructure}
\end{figure}

\subsection{深度学习的训练过程}

深度学习必然具有比较深的深度，也即比较多的层数，在按传统机器学习方法对深度学习模型进行训练会导致计算异常复杂，计算量变得十分巨大，如果一层一层的训练，又会出现偏差的逐层积累，导致偏差在训练中被不断放大，造成模型的欠拟合。2006年，Geoffrey Hinton教授给出了一种在非监督数据上建立多层神经网络的方法，Hinton教授给出的方法分为如下两个步骤：
\begin{enumerate}
    \item 自底向上的无监督学习

    自底向上的无监督学习，也是深度学习的特征学习过程，第一步，将待训练的训练数据集的特征与标签分离(如果是无标签数据，则不用进行这一步)；第二步，深度学习网络对深度学习网络中的第一层使用分离后无标签的训练数据进行训练，使用贪婪无监督的训练方法对其训练，第一层训练完毕后，将第一层训练的结果作为第二层的输入，再次使用上述的无监督学习方法进行训练，依此类推，直至深度学习网络的每一层都训练完毕，此时，我们已经得到了各层的参数了。

    \item 自顶向下的监督学习

    经过自底向上的无监督学习后，深度学习网络使用带标签的训练数据对网络进行进一步训练，由于使用了带有标签的数据对网络进行训练，这一步是监督学习的过程。在自底向上的无监督学习过程中，只能保证各层权值矩阵相对最优而达不到全局最优，所以在自顶向下的监督学习过程中可以将误差从上到下的传递，对每一层的权值矩阵进行微调，从而实现全局最优。那么如何调优呢？Hinton教授使用wake-sleep算法进行调优。先将除去最顶层的其他层之间的权值变为双向的，保证了最顶层是一个单层神经网络，但是其它层转为了图模型。向上的权值表示“认知”，向下的权值表示“生成”。下一步，使用wake-sleep算法调整所有的权值，使得“认知”和“生成”取得一致，这样就确保了生成的最顶层表示可以尽可能的复原底层的节点。例如，如果顶层的一个节点表示这是一个苹果，那么所有的关于苹果的图像就应该激活这个节点，同时这个结点向下生成的图像应该能看起来与苹果类似。
    wake-sleep算法分为wake与sleep两部分：
    \begin{enumerate}
        \item wake：认知过程，借助特征与向上的权值(认知权值)产生每一层的抽象表示，并且，使用梯度下降算法调整每层之间的权值(生成权值)。可以理解为：如果结果与我想象的不一样，改变我的权值使得我想象的结果变为这个样子。
        \item sleep：生成过程，借助顶层表示和向下的权值，生成下层的状态，同时修改每一层间向上的权值。可以理解为：如果通过结果推倒得到的我睡梦中的景象不是我在想象这个结果时产生的景象，那么应该改变我的认知权值使得这种景象在我想象时就是这个样子。
    \end{enumerate}
\end{enumerate}

\subsection{深度学习的优点}

深度学习具有强大的特征学习能力，深度学习的深层结构对复杂数据不仅能起到降维作用，还可以提炼出深层的特征，这意味着我们不需要花费成百上千小时来手工构建、筛选优秀的特征。在这之前，人们花费大量的时间在特征选择这项工作上，甚至有一门专业称为特征选择工程，可见，特征选择的困难是阻碍机器学习发展的一个大问题，而深度学习恰恰可以解决这种问题。另一方面，虽然有监督学习速度较快，效果也很好，但是当今人们每天从社交媒体、硬件和软件服务协议、应用程序许可以及网站cookie中收集到的大量信息都是无标记的，这些信息蕴含了很高的价值可是却没有办法对这些信息进行有效的标注。深度学习网络可以避免这种缺点，因为它在无监督学习中很有优势。

随着硬件方面的巨大进步，也进一步使深度学习成为可能。现代的GPU和TPU每秒可以执行超过万亿次的计算，这使得我们可以在不到一天的时间内训练好一个拥有数百万节点的深度网络。最近，Facebook的研究实验室表示，它们可以在一个小时内学习130万张图片，当然，这恐怖数字的背后需要庞大的计算基础设施支持，但也不难看出目前机器的学习能力是有多么的强大。

深度学习网络可以成功的应用于图像识别、模式识别、基于大数据的知识发现、知识应用和知识预测，而网络流量异常检测这一问题很长一段时间都依赖有专业经验的人手工编写识别规则，使用深度学习让机器自己发现网络流量异常检测规则对于传统方法而言是一个巨大的提升。

\section{网络流量异常检测}

\section{本章小结}
